{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "51cf86c5",
      "metadata": {
        "id": "51cf86c5"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf199e6a",
      "metadata": {
        "id": "bf199e6a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f56ecd",
      "metadata": {
        "id": "d8f56ecd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a897f2",
      "metadata": {
        "id": "38a897f2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-jar",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47900073",
      "metadata": {
        "id": "47900073",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-pyspark-import",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bed56b",
      "metadata": {
        "id": "72bed56b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-spark-version",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980e62a5",
      "metadata": {
        "id": "980e62a5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = '206921116_ass3' \n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if b.name.endswith('.parquet'):\n",
        "        paths.append(full_path+b.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481f2044",
      "metadata": {
        "id": "481f2044"
      },
      "source": [
        "read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c523e7",
      "metadata": {
        "id": "e4c523e7",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "#text:\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
        "\n",
        "#title:\n",
        "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
        "\n",
        "#anchor:\n",
        "doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n",
        "doc_anchor_pairs1 = doc_anchor_pairs.flatMap(lambda x: [(i['text'],i['id']) for i in x['anchor_text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VeQF4jOkQEnP",
      "metadata": {
        "id": "VeQF4jOkQEnP"
      },
      "source": [
        "tokenize functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d05c55d",
      "metadata": {
        "id": "4d05c55d"
      },
      "outputs": [],
      "source": [
        "#tokenize WITH STEMMER----------------------------------------------------------------------------------------------\n",
        "from nltk.stem import PorterStemmer\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = ['category', 'references', 'also', 'links', 'extenal', 'see', 'thumb']\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?[\\w,]?[\\w.]?(?:['\\-]?[\\w,]?[\\w])){0,24}\"\"\", re.UNICODE)\n",
        "RE_WORD_OLD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stopwords_frozen = frozenset(stopwords.words('english'))\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    res = []\n",
        "    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if\n",
        "                      token.group() not in all_stopwords]\n",
        "    for token in list_of_tokens:\n",
        "        res.append(stemmer.stem(token))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IqZoZ3fvGPZq",
      "metadata": {
        "id": "IqZoZ3fvGPZq"
      },
      "outputs": [],
      "source": [
        "def tokenize_no_stem(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    list_of_tokens = [token.group() for token in RE_WORD_OLD.finditer(text.lower()) if\n",
        "                      token.group() not in all_stopwords]\n",
        "    return list_of_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lw_7PIWKQKki",
      "metadata": {
        "id": "Lw_7PIWKQKki"
      },
      "source": [
        "**general**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qtWIuRSwQNuo",
      "metadata": {
        "id": "qtWIuRSwQNuo"
      },
      "source": [
        "NF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5211a8",
      "metadata": {
        "id": "de5211a8"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r gs://206921116_ass3/general/DL_dict_body.pickle .\n",
        "\n",
        "!gsutil -m cp -r gs://206921116_ass3/body_index ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3578ed",
      "metadata": {
        "id": "4c3578ed"
      },
      "outputs": [],
      "source": [
        "with open(Path('.') / f'DL_dict_body.pickle', 'rb') as f:\n",
        "    DL = pickle.load(f)\n",
        "body_index = InvertedIndex.read_index(\"body_index\", \"body_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lhC4MbaXGbcI",
      "metadata": {
        "id": "lhC4MbaXGbcI"
      },
      "outputs": [],
      "source": [
        "# NF for COSINE SIMILARITY WITH TFIDF on NON STEM body\n",
        "from collections import Counter\n",
        "import math\n",
        "def all_words_in_doc(text):\n",
        "  tokens = tokenize_no_stem(text)\n",
        "  return Counter(tokens)\n",
        "\n",
        "def get_nf(id, count):\n",
        "  length = len(DL)\n",
        "  total = 0\n",
        "  for token in count:\n",
        "    if token not in body_index.df:\n",
        "        continue\n",
        "    tf = count[token]/length\n",
        "    df = body_index.df[token]\n",
        "    idf = math.log(length/df,10)\n",
        "    tfidf = tf*idf\n",
        "    total += tfidf**2\n",
        "  return math.sqrt(total)\n",
        "\n",
        "res = doc_text_pairs.map(lambda x: (x[1],all_words_in_doc(x[0])))\n",
        "final_r = res.map(lambda x: (x[0], get_nf(x[0],x[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92ed770",
      "metadata": {
        "id": "e92ed770"
      },
      "outputs": [],
      "source": [
        "#collect final_r for pickle\n",
        "a=final_r.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jsEfrmZIOqVg",
      "metadata": {
        "id": "jsEfrmZIOqVg"
      },
      "outputs": [],
      "source": [
        "#write to bucket\n",
        "pickle.dump(final_r, open(\"doc_nf.pickle\", \"wb\"))\n",
        "!gsutil cp doc_nf.pickle gs://206921116_ass3/general/doc_nf.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Luo5SleqQfZd",
      "metadata": {
        "id": "Luo5SleqQfZd"
      },
      "source": [
        "DL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8GDba09rQgnF",
      "metadata": {
        "id": "8GDba09rQgnF"
      },
      "outputs": [],
      "source": [
        "## code for all DLs\n",
        "def len_of_doc(t):\n",
        "    x= tokenize(t)\n",
        "    return len(x)\n",
        "\n",
        "# DL for body\n",
        "a_body = doc_text_pairs.map(lambda x: (x[1], len_of_doc(x[0])))\n",
        "b_body = a.collectAsMap()\n",
        "\n",
        "#DL for title\n",
        "a = doc_title_pairs.map(lambda x: (x[1], len_of_doc(x[0])))\n",
        "b = a.collectAsMap()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F-ME-iE2QmTV",
      "metadata": {
        "id": "F-ME-iE2QmTV"
      },
      "outputs": [],
      "source": [
        "#download all\n",
        "#body\n",
        "pickle.dump(b_body, open(\"DL_dict_body.pickle\", \"wb\"))\n",
        "!gsutil cp DL_dict_body.pickle gs://206921116_ass3/general/DL_dict_body.pickle\n",
        "#title\n",
        "pickle.dump(b, open(\"DL_dict_title.pickle\", \"wb\"))\n",
        "!gsutil cp DL_dict_title.pickle gs://206921116_ass3/general/DL_dict_title.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9iL3GqGdRBKB",
      "metadata": {
        "id": "9iL3GqGdRBKB"
      },
      "source": [
        "doc_title_pairs_bykey -> get a title from an ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKngI5WJRAiN",
      "metadata": {
        "id": "UKngI5WJRAiN"
      },
      "outputs": [],
      "source": [
        "# create dict for id -> doc title - doesn't matter if STEM\n",
        "doc_title_pairs_bykey = doc_title_pairs1.sortByKey()\n",
        "b = doc_title_pairs_bykey.collectAsMap()\n",
        "\n",
        "#pickle and save in bucket\n",
        "pickle.dump(b, open(\"doc_title_pairs_bykey.pickle\", \"wb\"))\n",
        "!gsutil cp doc_title_pairs_bykey.pickle gs://206921116_ass3/general/doc_title_pairs_bykey.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_Lxv_NoyRQx0",
      "metadata": {
        "id": "_Lxv_NoyRQx0"
      },
      "source": [
        "import inverted index class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121fe102",
      "metadata": {
        "id": "121fe102"
      },
      "outputs": [],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c101a8",
      "metadata": {
        "id": "57c101a8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c259c402",
      "metadata": {
        "id": "c259c402"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ve-dfbXvRTmj",
      "metadata": {
        "id": "Ve-dfbXvRTmj"
      },
      "source": [
        "functions needed for all indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ad8fea",
      "metadata": {
        "id": "f3ad8fea",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#for all---------------------------------------\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = tokenize(text)\n",
        "  count_dict = {}\n",
        "  for token in tokens:\n",
        "    if token in all_stopwords:\n",
        "      continue\n",
        "    if token in count_dict:\n",
        "      count_dict[token] = count_dict[token]+1\n",
        "    else:\n",
        "      count_dict[token] = 1\n",
        "  res = []\n",
        "  added = []\n",
        "  for token in tokens:\n",
        "    if token in count_dict and token not in added:\n",
        "      res.append((token, (id, count_dict[token])))\n",
        "      added.append(token)\n",
        "  return res\n",
        "\n",
        "def word_count_no_stem(text, id):\n",
        "  tokens = tokenize_no_stem(text)\n",
        "  count_dict = {}\n",
        "  for token in tokens:\n",
        "    if token in all_stopwords:\n",
        "      continue\n",
        "    if token in count_dict:\n",
        "      count_dict[token] = count_dict[token]+1\n",
        "    else:\n",
        "      count_dict[token] = 1\n",
        "  res = []\n",
        "  added = []\n",
        "  for token in tokens:\n",
        "    if token in count_dict and token not in added:\n",
        "      res.append((token, (id, count_dict[token])))\n",
        "      added.append(token)\n",
        "  return res\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl)\n",
        "\n",
        "def calculate_df(postings):\n",
        "  new_posting = postings.map(lambda x:(x[0],len(x[1])))\n",
        "  return new_posting\n",
        "\n",
        "def partition_postings_and_write(postings, index_name):\n",
        "  new_rdd = postings.groupBy(lambda x: token2bucket_id(x[0]))\n",
        "  final_rdd = new_rdd.map(lambda x: InvertedIndex.write_a_posting_list(x, index_name,bucket_name))\n",
        "  return final_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KkLbo9MpRbLG",
      "metadata": {
        "id": "KkLbo9MpRbLG"
      },
      "source": [
        "**body index**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JL3WpwwNRgOK",
      "metadata": {
        "id": "JL3WpwwNRgOK"
      },
      "source": [
        "STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c8764e",
      "metadata": {
        "id": "55c8764e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#text-----------------------------------------------------------------------\n",
        "# time the index creation time\n",
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_filtered, \"stem_body_index\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3296f4",
      "metadata": {
        "id": "ab3296f4",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#text-----------------------------------------\n",
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='stem_body_index'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  if \"body\" not in blob.name:\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd9108f",
      "metadata": {
        "id": "ebd9108f"
      },
      "outputs": [],
      "source": [
        "#text--------------------------------------------------------------------------\n",
        "# Create inverted index instance\n",
        "inverted_text = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_text.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_text.df = w2df_dict\n",
        "# write the global stats out\n",
        "path = f'gs://{bucket_name}/stem_body_index'\n",
        "inverted_text.write_index('.', 'stem_body_index')\n",
        "# upload to gs\n",
        "index_src = \"stem_body_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/stem_body_index/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aR_kdOmjRlOg",
      "metadata": {
        "id": "aR_kdOmjRlOg"
      },
      "source": [
        "NON STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vBHPeGZERkWd",
      "metadata": {
        "id": "vBHPeGZERkWd"
      },
      "outputs": [],
      "source": [
        "#text NON STEM-----------------------------------------------------------------------\n",
        "# time the index creation time\n",
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count_no_stem(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_filtered, \"body_index\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j5zqJHN_RoJe",
      "metadata": {
        "id": "j5zqJHN_RoJe"
      },
      "outputs": [],
      "source": [
        "#text-----------------------------------------\n",
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='body_index'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  if \"body\" not in blob.name:\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kZe8cQhLRpuF",
      "metadata": {
        "id": "kZe8cQhLRpuF"
      },
      "outputs": [],
      "source": [
        "#text--------------------------------------------------------------------------\n",
        "# Create inverted index instance\n",
        "inverted_text = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_text.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_text.df = w2df_dict\n",
        "# write the global stats out\n",
        "path = f'gs://{bucket_name}/body_index'\n",
        "inverted_text.write_index('.', 'body_index')\n",
        "# upload to gs\n",
        "index_src = \"body_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/body_index/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mc4jRAzsR-gG",
      "metadata": {
        "id": "mc4jRAzsR-gG"
      },
      "source": [
        "TITLE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oVvQaG19SAeX",
      "metadata": {
        "id": "oVvQaG19SAeX"
      },
      "source": [
        "STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c1f4bb",
      "metadata": {
        "id": "33c1f4bb"
      },
      "outputs": [],
      "source": [
        "#title-----------------------------------------------------------------------\n",
        "# time the index creation time\n",
        "# word counts map\n",
        "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "w2df_title = calculate_df(postings_title)\n",
        "w2df_dict_title = w2df_title.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_title, \"stem_title_index\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f30d1d6",
      "metadata": {
        "id": "7f30d1d6"
      },
      "outputs": [],
      "source": [
        "#title-----------------------------------------\n",
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_title = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='stem_title_index'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs_title[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d2cfb6",
      "metadata": {
        "id": "a5d2cfb6"
      },
      "outputs": [],
      "source": [
        "#title---------------------------------------------------------------------------\n",
        "# Create inverted index instance\n",
        "inverted_title = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_title.posting_locs = super_posting_locs_title\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_title.df = w2df_dict_title\n",
        "# write the global stats out\n",
        "inverted_title.write_index('.', 'stem_title_index')\n",
        "# upload to gs\n",
        "index_src = \"stem_title_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/stem_title_index/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fj7xeV6VSE5M",
      "metadata": {
        "id": "fj7xeV6VSE5M"
      },
      "source": [
        "NO STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5957bFJKSHIx",
      "metadata": {
        "id": "5957bFJKSHIx"
      },
      "outputs": [],
      "source": [
        "#title-----------------------------------------------------------------------\n",
        "# time the index creation time\n",
        "# word counts map\n",
        "word_counts_title = doc_title_pairs.flatMap(lambda x: word_count_no_stem(x[0], x[1]))\n",
        "postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "w2df_title = calculate_df(postings_title)\n",
        "w2df_dict_title = w2df_title.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_title, \"title_index\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YyHLtamYSHCE",
      "metadata": {
        "id": "YyHLtamYSHCE"
      },
      "outputs": [],
      "source": [
        "#title-----------------------------------------\n",
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_title = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='title_index'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs_title[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecL6utTSSG54",
      "metadata": {
        "id": "ecL6utTSSG54"
      },
      "outputs": [],
      "source": [
        "#title---------------------------------------------------------------------------\n",
        "# Create inverted index instance\n",
        "inverted_title = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_title.posting_locs = super_posting_locs_title\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_title.df = w2df_dict_title\n",
        "# write the global stats out\n",
        "inverted_title.write_index('.', 'title_index')\n",
        "# upload to gs\n",
        "index_src = \"title_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YVUOUheySVIG",
      "metadata": {
        "id": "YVUOUheySVIG"
      },
      "source": [
        "ANCHOR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Thmq9xyaSWi1",
      "metadata": {
        "id": "Thmq9xyaSWi1"
      },
      "source": [
        "NO STEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hpLXKM2wPRaP",
      "metadata": {
        "id": "hpLXKM2wPRaP"
      },
      "outputs": [],
      "source": [
        "#anchor NOT STEM------------\n",
        "x=doc_anchor_pairs1.flatMap(lambda x: [(i, x[1]) for i in tokenize_no_stem(x[0])])\n",
        "y=x.groupByKey().mapValues(list)\n",
        "z=y.map(lambda x: (x[0],list(Counter(x[1]).items())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b068dccc",
      "metadata": {
        "id": "b068dccc"
      },
      "outputs": [],
      "source": [
        "#anchor NO STEM----------------------------------------------------------------------\n",
        "w2df_anch=calculate_df(z).collectAsMap()\n",
        "posting_locs_list_anch = partition_postings_and_write(z, \"anchor_index\").collect()\n",
        "\n",
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs_anchor = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='anchor_index'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  if \"anchor\" not in blob.name:\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs_anchor[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e835124",
      "metadata": {
        "id": "4e835124"
      },
      "outputs": [],
      "source": [
        "#anchor NO STEM--------------------------------------------------------------------------\n",
        "# Create inverted index instance\n",
        "inverted_anch = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted_anch.posting_locs = super_posting_locs_anchor\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted_anch.df = w2df_anch\n",
        "# write the global stats out\n",
        "inverted_anch.write_index('.', 'anchor_index')\n",
        "# upload to gs\n",
        "index_src = \"anchor_index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LAEZYZToSmvU",
      "metadata": {
        "id": "LAEZYZToSmvU"
      },
      "source": [
        "example of how to download locally from bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d8ce90",
      "metadata": {
        "id": "69d8ce90"
      },
      "outputs": [],
      "source": [
        "!gsutil cp -r gs://206921116_ass3/anchor_index . # download all anchor folder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6lpY2mGWSwSb",
      "metadata": {
        "id": "6lpY2mGWSwSb"
      },
      "source": [
        "example of how to open a pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uy5ID5klSz42",
      "metadata": {
        "id": "uy5ID5klSz42"
      },
      "outputs": [],
      "source": [
        "with open(Path(\"general\") / f'page_rank_dict.pickle', 'rb') as f:\n",
        "    pr = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iEu7FYhFS1qt",
      "metadata": {
        "id": "iEu7FYhFS1qt"
      },
      "source": [
        "read_posting_list  function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50cc6c1b",
      "metadata": {
        "id": "50cc6c1b"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import MultiFileReader\n",
        "from inverted_index_gcp import MultiFileWriter\n",
        "\n",
        "TUPLE_SIZE = 6       \n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "from contextlib import closing\n",
        "\n",
        "def read_posting_list(inverted, w, index_name):\n",
        "  with closing(MultiFileReader()) as reader:\n",
        "    locs = inverted.posting_locs[w]\n",
        "    b = reader.read(locs, inverted.df[w] * TUPLE_SIZE, index_name)\n",
        "    posting_list = []\n",
        "    for i in range(inverted.df[w]):\n",
        "      doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
        "      tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
        "      posting_list.append((doc_id, tf))\n",
        "    return posting_list\n",
        "\n",
        "\n",
        "#pl = read_posting_list(inverted_anch, 'horse', \"anchor_index\")\n",
        "#print(pl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w0MNSOIuRKlL",
      "metadata": {
        "id": "w0MNSOIuRKlL"
      },
      "source": [
        "page rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e96c0f",
      "metadata": {
        "id": "a2e96c0f"
      },
      "outputs": [],
      "source": [
        "#graph for page rank--------------------------------------------------------------------------------\n",
        "\n",
        "parquetFile = spark.read.parquet(*paths)\n",
        "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "\n",
        "edges= pages_links.flatMap(lambda x: [(x[0], w[0]) for w in x[1]]).distinct()\n",
        "vertices= edges.flatMap(lambda x: [(x[0],),(x[1],)]).distinct()\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "#pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
        "pr.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafe8a5e",
      "metadata": {
        "id": "cafe8a5e"
      },
      "outputs": [],
      "source": [
        "#convert df to rdd and create dict\n",
        "pr_rdd = pr.rdd\n",
        "pr_dict = pr_rdd.collectAsMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864649b3",
      "metadata": {
        "id": "864649b3"
      },
      "outputs": [],
      "source": [
        "# write page rank dict to bucket\n",
        "pickle.dump(pr_dict, open(\"page_rank_dict.pickle\", \"wb\"))\n",
        "!gsutil cp page_rank_dict.pickle gs://206921116_ass3/general/page_rank_dict.pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7CT6MyOnRM3g",
      "metadata": {
        "id": "7CT6MyOnRM3g"
      },
      "source": [
        "page view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f74693",
      "metadata": {
        "id": "36f74693"
      },
      "outputs": [],
      "source": [
        "#for page_view--------------------------------------------------------------------------------\n",
        "# Paths\n",
        "# Using user page views (as opposed to spiders and automated traffic) for the \n",
        "# month of August 2021\n",
        "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
        "p = Path(pv_path) \n",
        "pv_name = p.name\n",
        "pv_temp = f'{p.stem}-4dedup.txt'\n",
        "pv_clean = f'{p.stem}.pkl'\n",
        "# Download the file (2.3GB) \n",
        "!wget -N $pv_path\n",
        "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
        "# total number of page views (5). Then, remove lines with article id or page \n",
        "# view values that are not a sequence of digits.\n",
        "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        "# Create a Counter (dictionary) that sums up the pages views for the same \n",
        "# article, resulting in a mapping from article id to total page views.\n",
        "wid2pv = Counter()\n",
        "with open(pv_temp, 'rt') as f:\n",
        "  for line in f:\n",
        "    parts = line.split(' ')\n",
        "    wid2pv.update({int(parts[0]): int(parts[1])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf2ebf6",
      "metadata": {
        "id": "bdf2ebf6"
      },
      "outputs": [],
      "source": [
        "pickle.dump(wid2pv, open(\"page_view_stats.pickle\", \"wb\"))\n",
        "!gsutil cp page_view_stats.pickle gs://206921116_ass3/general/page_view_stats.pickle"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}